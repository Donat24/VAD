{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jonas\\Documents\\Masterarbeit\\Scripte\\Labeling\\LABELENV\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import huggingsound\n",
    "from huggingsound import SpeechRecognitionModel\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import ast\n",
    "import librosa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erstellen der CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lädt Referenz\n",
    "path_csv     = r\"D:\\Masterarbeit\\LABELD\\test.csv\"\n",
    "csv          = pd.read_csv(path_csv)\n",
    "\n",
    "#Lädt Datein\n",
    "folder_test  = r\"D:\\Masterarbeit\\SAMPLES PROCESSED\\Voice\\TEST\"\n",
    "files_test   = csv.filename.apply(lambda filename: os.path.join(folder_test,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/31/2023 15:57:12 - INFO - huggingsound.speech_recognition.model - Loading model...\n"
     ]
    }
   ],
   "source": [
    "#Lädt Model\n",
    "model = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [26:14<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "result = model.transcribe(files_test)\n",
    "csv[\"result_wav2vec2\"] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.to_csv(\"result_wav2vec2.csv\",index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auswertung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"result_wav2vec2.csv\")\n",
    "csv[\"result_wav2vec2\"] = csv.result_wav2vec2.apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_wav2vec2 = csv.result_wav2vec2.apply(lambda json: min(json[\"start_timestamps\"]) / 1000)\n",
    "end_wav2vec2   = csv.result_wav2vec2.apply(lambda json: max(json[\"end_timestamps\"])   / 1000)\n",
    "\n",
    "diff_start = np.abs((start_wav2vec2 - csv.start) * 1000)\n",
    "diff_end   = np.abs((end_wav2vec2 - csv.end) * 1000)\n",
    "\n",
    "total_diff = diff_start + diff_end\n",
    "csv[\"total_diff_wav2vec2\"] = total_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155.654296875"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv[\"total_diff_wav2vec2\"].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Über DB Treshhold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Konstante\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "#Funktion lädt Tensor\n",
    "def load_file_as_tensor(file_path):\n",
    "\n",
    "    waveform, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE, mono=True, dtype=\"float64\")\n",
    "    \n",
    "    #Tensor\n",
    "    waveform = torch.from_numpy(waveform).to(torch.float32)\n",
    "\n",
    "    #Return\n",
    "    return waveform, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erstellt Pfad\n",
    "folder_test  = r\"D:\\Masterarbeit\\SAMPLES PROCESSED\\Voice\\TEST\"\n",
    "files_test   = csv.filename.apply(lambda filename: os.path.join(folder_test,filename))\n",
    "\n",
    "#Lädt Datein\n",
    "waveforms_samplerates = files_test.apply(load_file_as_tensor)\n",
    "waveforms = waveforms_samplerates.apply(lambda ws: ws[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AMP to DB und umgekehrt\n",
    "def amp_to_db(tensor):\n",
    "    return tensor.log10() * 20\n",
    "\n",
    "def db_to_amp(tensor):\n",
    "    return 10.0**(0.5 * tensor/10)\n",
    "\n",
    "#Berechnet RMS\n",
    "def rms(tensor):\n",
    "    tensor = tensor.square()\n",
    "    tensor = tensor.mean(dim=-1)\n",
    "    tensor = tensor.sqrt()\n",
    "    return tensor\n",
    "\n",
    "def db(tensor):\n",
    "    return amp_to_db(rms(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Konstanten\n",
    "NON_SILENT_SAMPLE_LENGTH = 0.03\n",
    "NON_SILENT_HOP_LENGTH    = 0.01\n",
    "MAX_GAP_LENGTH           = 0.6\n",
    "\n",
    "NON_SILENT_SAMPLE_LENGTH   = librosa.time_to_samples(NON_SILENT_SAMPLE_LENGTH, sr=SAMPLE_RATE)\n",
    "NON_SILENT_HOP_LENGTH      = librosa.time_to_samples(NON_SILENT_HOP_LENGTH,    sr=SAMPLE_RATE)\n",
    "MAX_GAP_LENGTH             = librosa.time_to_samples(MAX_GAP_LENGTH,           sr=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erzeugt Matrix welche zum Mulitplizieren verwendet wird\n",
    "matrix_m_number_of_rows  = 1000\n",
    "matrix_n_waveform_length = librosa.time_to_samples(15,sr=SAMPLE_RATE)\n",
    "\n",
    "multiplication_matix = []\n",
    "\n",
    "for idx in range(matrix_m_number_of_rows):\n",
    "    \n",
    "    #erzeugt neue Zeile\n",
    "    row = torch.zeros(matrix_n_waveform_length)\n",
    "    \n",
    "    #Setzt der 1en\n",
    "    current_pos = idx * NON_SILENT_HOP_LENGTH\n",
    "    row[current_pos:current_pos + NON_SILENT_SAMPLE_LENGTH] = 1\n",
    "\n",
    "    multiplication_matix.append(row)\n",
    "\n",
    "multiplication_matix = torch.vstack(multiplication_matix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_non_silent(waveform, treshhold = 20):\n",
    "\n",
    "    #Sichert das der Übergebense Tensor die Form[Datenpunkt]\n",
    "    if len(waveform.shape) != 1:\n",
    "        raise Exception(\"BAD TENSOR SHAPE\")\n",
    "    \n",
    "    #Speichert Wert für Später\n",
    "    waveform_length = waveform.shape[-1]\n",
    "\n",
    "    #Fügt 0 am Tensor an damit über die Gesamte Länge die Sprache erkannt werden kann\n",
    "    waveform = torch.cat([waveform, torch.zeros(NON_SILENT_SAMPLE_LENGTH - 1)])\n",
    "\n",
    "    #Transformiert in Shape [X, NON_SILENT_SAMPLE_LENGTH]\n",
    "    waveform = waveform.unfold(\n",
    "        dimension = 0,\n",
    "        size      = NON_SILENT_SAMPLE_LENGTH,\n",
    "        step      = NON_SILENT_HOP_LENGTH\n",
    "    )\n",
    "\n",
    "    #Berechnet DBFS\n",
    "    waveform_db = db(waveform)\n",
    "\n",
    "    #Treshhold\n",
    "    waveform_db.gt_(waveform_db.max() - treshhold)\n",
    "\n",
    "    #Mulitpliziert mit Matrix um Abdeckung zu vergleichen\n",
    "    waveform_db = waveform_db.diag()\n",
    "    result   = torch.matmul(waveform_db, multiplication_matix[:waveform_db.shape[-1], :waveform_length].to(waveform.dtype))\n",
    "\n",
    "    return result.sum(dim=0).gt_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_silent = waveforms.apply(lambda waveform: get_non_silent(waveform))\n",
    "nonzero    = non_silent.apply(lambda tensor: tensor.flatten().nonzero().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kalkuliert Start und Endpunkte\n",
    "start_selfmade = nonzero.apply(lambda tensor: tensor.min().item())\n",
    "start_selfmade = start_selfmade.apply(lambda startpoint: librosa.samples_to_time(samples=startpoint,sr=SAMPLE_RATE))\n",
    "\n",
    "end_selfmade   = nonzero.apply(lambda tensor: tensor.max().item())\n",
    "end_selfmade   = end_selfmade.apply(lambda end_selfmade: librosa.samples_to_time(samples=end_selfmade,sr=SAMPLE_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_start = np.abs((start_selfmade - csv.start) * 1000)\n",
    "diff_end   = np.abs((end_wav2vec2 - csv.end) * 1000)\n",
    "\n",
    "total_diff = diff_start + diff_end\n",
    "csv[\"total_diff_selfmade\"] = total_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219.443359375"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv[\"total_diff_selfmade\"].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kombination aus beidem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_duration = files_test.apply(lambda filepath: librosa.get_duration(path=filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_window_size = 0.2\n",
    "\n",
    "#Start\n",
    "search_start_lower = start_wav2vec2 - search_window_size / 2\n",
    "search_start_lower[search_start_lower < 0] = 0\n",
    "search_start_upper = start_wav2vec2 + search_window_size / 2\n",
    "search_start_upper[search_start_upper > files_duration] = files_duration[search_start_upper > files_duration]\n",
    "\n",
    "#Ende\n",
    "search_end_lower = end_wav2vec2 - search_window_size / 2\n",
    "search_end_lower[search_start_lower < 0] = 0\n",
    "search_end_upper = end_wav2vec2 + search_window_size / 2\n",
    "search_end_upper[search_end_upper > files_duration] = files_duration[search_end_upper > files_duration]\n",
    "\n",
    "#to Frames\n",
    "search_start_lower = search_start_lower.apply(lambda time: librosa.time_to_samples(times=time,sr=SAMPLE_RATE))\n",
    "search_start_upper = search_start_upper.apply(lambda time: librosa.time_to_samples(times=time,sr=SAMPLE_RATE))\n",
    "search_end_lower   = search_end_lower.apply(lambda time: librosa.time_to_samples(times=time,sr=SAMPLE_RATE))\n",
    "search_end_upper   = search_end_upper.apply(lambda time: librosa.time_to_samples(times=time,sr=SAMPLE_RATE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "_calc = pd.DataFrame({\n",
    "    \"nonzero\": nonzero,\n",
    "    \"search_start_lower\": search_start_lower,\n",
    "    \"search_start_upper\": search_start_upper,\n",
    "    \"w2v_start\"         : start_wav2vec2.apply(lambda time: librosa.time_to_samples(times=time,sr=SAMPLE_RATE)).apply(torch.tensor),\n",
    "\n",
    "    \"search_end_lower\": search_end_lower,\n",
    "    \"search_end_upper\": search_end_upper,\n",
    "    \"w2v_end\"         : end_wav2vec2.apply(lambda time: librosa.time_to_samples(times=time,sr=SAMPLE_RATE)).apply(torch.tensor),\n",
    "})\n",
    "\n",
    "calc_start = _calc.apply(lambda row: row[\"nonzero\"][ torch.logical_and (row[\"nonzero\"] >= row[\"search_start_lower\"], row[\"nonzero\"] <= row[\"search_start_upper\"]) ] ,axis=1)\n",
    "calc_start[calc_start.apply(lambda t: t.shape[0]) == 0] = _calc[calc_start.apply(lambda t: t.shape[0]) == 0].w2v_start\n",
    "calc_end   = _calc.apply(lambda row: row[\"nonzero\"][ torch.logical_and (row[\"nonzero\"] >= row[\"search_end_lower\"],   row[\"nonzero\"] <= row[\"search_end_upper\"]) ] ,axis=1)\n",
    "calc_end[calc_end.apply(lambda t: t.shape[0]) == 0] = _calc[calc_end.apply(lambda t: t.shape[0]) == 0].w2v_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_start = calc_start.apply(lambda t: t.min()).apply(lambda t: librosa.samples_to_time(samples=t,sr=SAMPLE_RATE))\n",
    "calc_end   = calc_end.apply(lambda t: t.max()).apply(lambda t: librosa.samples_to_time(samples=t,sr=SAMPLE_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110.37896728515625"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_start = np.abs((calc_start - csv.start) * 1000)\n",
    "diff_end   = np.abs((calc_end - csv.end) * 1000)\n",
    "\n",
    "total_diff = diff_start + diff_end\n",
    "csv[\"total_diff_combined\"] = total_diff\n",
    "csv[\"total_diff_combined\"].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LABELENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71340d0969d1bd12d49f5869400cd8685e4e2e16c63909a2ce3792bb16089c88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
