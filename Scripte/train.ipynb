{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python39\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Program Files\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Program Files\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import Audio\n",
    "from IPython.display import clear_output, display\n",
    "import librosa\n",
    "import math\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "#Eigene Sachen\n",
    "from train import train_model\n",
    "from data.data import *\n",
    "from util.util import *\n",
    "from util.datasets import *\n",
    "from util.audio_processing import *\n",
    "import util.plot as plot\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: lion-pytorch in /home/ogu/.local/lib/python3.10/site-packages (0.0.7)\n",
      "Requirement already satisfied: torch>=1.6 in /home/ogu/.local/lib/python3.10/site-packages (from lion-pytorch) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/ogu/.local/lib/python3.10/site-packages (from torch>=1.6->lion-pytorch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/ogu/.local/lib/python3.10/site-packages (from torch>=1.6->lion-pytorch) (11.7.99)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/ogu/.local/lib/python3.10/site-packages (from torch>=1.6->lion-pytorch) (2.0.0)\n",
      "Requirement already satisfied: filelock in /home/ogu/.local/lib/python3.10/site-packages (from torch>=1.6->lion-pytorch) (3.10.7)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/ogu/.local/lib/python3.10/site-packages (from torch>=1.6->lion-pytorch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/ogu/.local/lib/python3.10/site-packages (from torch>=1.6->lion-pytorch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/ogu/.local/lib/python3.10/site-packages (from torch>=1.6->lion-pytorch) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/ogu/.local/lib/python3.10/site-packages (from torch>=1.6->lion-pytorch) (11.4.0.1)\n",
      "Requirement already satisfied: networkx in /home/ogu/.local/lib/python3.10/site-packages (from torch>=1.6->lion-pytorch) (3.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/ogu/.local/lib/python3.10/site-packages (from torch>=1.6->lion-pytorch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/ogu/.local/lib/python3.10/site-packages (from torch>=1.6->lion-pytorch) (11.10.3.66)\n",
      "Requirement already satisfied: sympy in /home/ogu/.local/lib/python3.10/site-packages (from torch>=1.6->lion-pytorch) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/ogu/.local/lib/python3.10/site-packages (from torch>=1.6->lion-pytorch) (10.9.0.58)\n",
      "Requirement already satisfied: typing-extensions in /home/ogu/.local/lib/python3.10/site-packages (from torch>=1.6->lion-pytorch) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/ogu/.local/lib/python3.10/site-packages (from torch>=1.6->lion-pytorch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/ogu/.local/lib/python3.10/site-packages (from torch>=1.6->lion-pytorch) (10.2.10.91)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=1.6->lion-pytorch) (3.0.3)\n",
      "Requirement already satisfied: wheel in /home/ogu/.local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6->lion-pytorch) (0.40.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6->lion-pytorch) (59.6.0)\n",
      "Requirement already satisfied: cmake in /home/ogu/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6->lion-pytorch) (3.26.1)\n",
      "Requirement already satisfied: lit in /home/ogu/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6->lion-pytorch) (16.0.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ogu/.local/lib/python3.10/site-packages (from sympy->torch>=1.6->lion-pytorch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install lion-pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lion_pytorch import Lion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(torch.nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.module(inputs) + inputs\n",
    "\n",
    "\n",
    "class SineLayer(torch.nn.Module):\n",
    "    def __init__(self, in_features,out_features,dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.sequence = nn.Sequential(\n",
    "            ResNet(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_features, out_features),\n",
    "                    nn.SELU(),\n",
    "                    nn.BatchNorm1d(out_features),\n",
    "                    nn.Dropout(dropout)            \n",
    "              ))#,              \n",
    "              #nn.LayerNorm(out_features)\n",
    "        )        \n",
    "    def forward(self, inputs):\n",
    "        #return torch.sin(self.sequence(inputs))\n",
    "        #return self.sequence(torch.sin(inputs))\n",
    "        return self.sequence(inputs)\n",
    "\n",
    "class SinNetLayerNorm(models.LightningBase):\n",
    "    def __init__(self, input_size, dropout=0.1) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            SineLayer(input_size,input_size,dropout),\n",
    "            SineLayer(input_size,input_size,dropout),\n",
    "            SineLayer(input_size,input_size,dropout),\n",
    "            SineLayer(input_size,input_size,dropout),                        \n",
    "        )\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(nn.Linear(input_size, 1),                      \n",
    "                      #nn.BatchNorm1d(1),\n",
    "                      ##nn.Sigmoid(),\n",
    "                      #nn.LayerNorm(10),\n",
    "                      #nn.LogSoftmax(dim=1)\n",
    "                      )\n",
    "\n",
    "    def forward(self, x):\n",
    "       # x = x.view(x.shape[0], -1)\n",
    "        x = self.net(x)        \n",
    "        #x2 = self.net2(x)           \n",
    "        return self.decoder(x).view(-1)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Lion(self.parameters(), lr=1e-2)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttFunction(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_f):\n",
    "        super().__init__()\n",
    "        self.function = nn.Sequential(\n",
    "            nn.Linear(dim_in, dim_f, bias=False),            \n",
    "            nn.BatchNorm1d(dim_f),\n",
    "            #nn.LayerNorm(dim_f),\n",
    "            nn.ReLU(),            \n",
    "            nn.Dropout(0.1)\n",
    "            )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.function(inputs)\n",
    "\n",
    "class AttNet(models.LightningBase):\n",
    "    def __init__(self,dim_in,dim_f,dim_out,number_f=10):\n",
    "        super(AttNet, self).__init__()\n",
    "        self.to_k = nn.Linear(dim_in, number_f, bias=False)\n",
    "        self.number_f = number_f\n",
    "        # move this to module \n",
    "        # add two layer setup\n",
    "        self.to_f_1 = AttFunction(dim_in, dim_f)\n",
    "        self.to_f_2 = AttFunction(dim_in, dim_f)\n",
    "        self.to_f_3 = AttFunction(dim_in, dim_f)\n",
    "        self.to_f_4 = AttFunction(dim_in, dim_f)\n",
    "        self.to_f_5 = AttFunction(dim_in, dim_f)\n",
    "        self.to_f_6 = AttFunction(dim_in, dim_f)\n",
    "        self.to_f_7 = AttFunction(dim_in, dim_f)\n",
    "        self.to_f_8 = AttFunction(dim_in, dim_f)\n",
    "        self.to_f_9 = AttFunction(dim_in, dim_f)\n",
    "        self.to_f_10= AttFunction(dim_in, dim_f)\n",
    "        self.out = nn.Linear(dim_f, dim_out, bias=True)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        #k = self.to_k(x)\n",
    "        k = self.softmax(self.to_k(x))\n",
    "        k = k.view(-1,self.number_f,1)\n",
    "\n",
    "        f_1 = self.to_f_1(x)\n",
    "        f_2 = self.to_f_2(x)\n",
    "        f_3 = self.to_f_3(x)\n",
    "        f_4 = self.to_f_4(x)\n",
    "        f_5 = self.to_f_5(x)\n",
    "        f_6 = self.to_f_6(x)\n",
    "        f_7 = self.to_f_7(x)\n",
    "        f_8 = self.to_f_8(x)\n",
    "        f_9 = self.to_f_9(x)\n",
    "        f_10 =self.to_f_10(x)\n",
    "\n",
    "        f = torch.stack([f_1,f_2,f_3,f_4,f_5,f_6,f_7,f_8,f_9,f_10],dim=1)\n",
    "        f = f * k\n",
    "        f = f.sum(dim=1)\n",
    "        out = self.out(f).view(-1)\n",
    "        return out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Lion(self.parameters(), lr=1e-5)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Demo(models.LightningBase):\n",
    "    def __init__(self, input_size) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.fc2 = nn.Linear(512,512)\n",
    "        self.fc3 = nn.Linear(512,128)\n",
    "        self.fc4 = nn.Linear(128,64)\n",
    "        self.fc5 = nn.Linear(64,1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #forward\n",
    "        out = torch.relu(self.fc1(x))\n",
    "        out = torch.relu(self.fc2(out))\n",
    "        out = torch.relu(self.fc3(out))\n",
    "        out = torch.relu(self.fc4(out))\n",
    "        out = self.fc5(out)\n",
    "\n",
    "        return out.view(out.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoCNN(models.LightningBase):\n",
    "    def __init__(self, input_size) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.cnn = models.Net1D(\n",
    "        in_channels=1,\n",
    "        base_filters=64,\n",
    "        ratio=1.0,\n",
    "        filter_list = [64, 160, 160, 400, 1024],\n",
    "        m_blocks_list = [2, 2, 2, 3, 4],\n",
    "        kernel_size=16,\n",
    "        stride=2,\n",
    "        groups_width=16,\n",
    "        verbose=True,\n",
    "        n_classes=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #forward\n",
    "        out = self.cnn(x)\n",
    "\n",
    "        return out.view(out.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
=======
      "Using 16bit Automatic Mixed Precision (AMP)\n",
>>>>>>> Stashed changes
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
<<<<<<< Updated upstream
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type           | Params\n",
      "---------------------------------------------------\n",
      "0 | loss_fn         | BCELoss        | 0     \n",
      "1 | accuracy        | BinaryAccuracy | 0     \n",
      "2 | stft            | STFT           | 0     \n",
      "3 | first_cnn_layer | Block          | 544   \n",
      "4 | block_list      | ModuleList     | 16.5 K\n",
      "5 | last_cnn_layer  | Block          | 16.5 K\n",
      "6 | fc1             | Linear         | 1.1 K \n",
      "7 | bn1             | BatchNorm1d    | 64    \n",
      "8 | fc2             | Linear         | 33    \n",
      "---------------------------------------------------\n",
      "34.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "34.7 K    Total params\n",
      "0.139     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855253d5e9964b4eab8b668d5845e806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonas\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Jonas\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Jonas\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d742ca29b04e4c958702fbc8e2e3fa2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9843f5a481734a7fa642bf444adad6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d9960f64a948c4a851fcc8e89d08d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e6cf0c15c040b885926acd6cc8d28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f9333627644d5d86710ba321ef696b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b863189b4b5f4fe998259e53f4e6abd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bdbebf19d554c6dba8640c9a5233e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e96165e061411d9226dfeb3ce30295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0d454719f14b6ba4e90bd85577cf73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34cf5cdcd4942f1987f804b7fe621c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9ecefa37164c5c8b737674af237f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8109053ebfbd4c43b2d523d5b61c18ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac734da0d1ad4573a03f14f46274d3f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19fcff16d2b4eb5a16ca510b141a985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ea5d3a8ac946dbada015080c94f4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc42280be31241f0b867f2dad2f0385f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fbd2672391941f5ad92ba2bdd466c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9abf583e5b4525b6a80c1264eb6fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55cba78ce6794de998c19a92cf906fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0941fc275e4aa2b01dd4759d5444b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36336be4cbaf4e1cae91d1532268b911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bdd053bb2b9439cb0cb1a4f899afe61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba1e77b9e764fcc98822c8ca12b5b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonas\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "#Lädt Model\n",
    "model = models.STFTPuffered()\n",
    "train_model(model)"
=======
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name     | Type           | Params\n",
      "---------------------------------------------\n",
      "0  | accuracy | BinaryAccuracy | 0     \n",
      "1  | conv1    | Conv1d         | 16.4 K\n",
      "2  | bn1      | BatchNorm1d    | 128   \n",
      "3  | pool1    | MaxPool1d      | 0     \n",
      "4  | conv2    | Conv1d         | 16.4 K\n",
      "5  | bn2      | BatchNorm1d    | 128   \n",
      "6  | pool2    | MaxPool1d      | 0     \n",
      "7  | conv3    | Conv1d         | 8.3 K \n",
      "8  | bn3      | BatchNorm1d    | 128   \n",
      "9  | pool3    | MaxPool1d      | 0     \n",
      "10 | fc1      | Linear         | 65    \n",
      "---------------------------------------------\n",
      "41.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "41.6 K    Total params\n",
      "0.166     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 38089/38089 [13:48<00:00, 46.00it/s, v_num=11]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 38089/38089 [13:48<00:00, 46.00it/s, v_num=11]\n"
     ]
    }
   ],
   "source": [
    "#Lädt Model\n",
    "torch.set_float32_matmul_precision('high')\n",
    "model = models.CNN(channels=128)\n",
    "#compiled_model = torch.compile(model) -> does leed to bugs -> update dependencies?\n",
    "\n",
    "#model = models.DeepCNN(n_blocks=2)\n",
    "\n",
    "#model = AttNet(1024,100,1) \n",
    "#model = SinNetLayerNorm(512)\n",
    "train_model(model,max_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lädt Model\n",
    "#model = models.MLP(input_size = SAMPLE_LENGTH)\n",
    "#train_model(model,max_steps=-1)"
>>>>>>> Stashed changes
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< Updated upstream
   "display_name": "Python 3",
=======
   "display_name": "Python 3 (ipykernel)",
>>>>>>> Stashed changes
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
